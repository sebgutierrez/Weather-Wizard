{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the cdsapi package to download the data. We will need xarray to concatenate the data. netCDF4, eccodes, cfgrib, ecmwflibs, and netcdf4 are needed to open the GRIB file and also convert to NetCDF.\n",
    "\n",
    "When you wish to execute shell commands in a jupyter notebook, start the command with \"!\". I am using a virtual environment to run this notebook, so it won't install the packages globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdsapi in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: xarray in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2023.10.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: eccodes in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: cfgrib in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.9.10.4)\n",
      "Requirement already satisfied: ecmwflibs in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: netcdf4 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ecmfw (from versions: none)\n",
      "ERROR: No matching distribution found for ecmfw\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install cdsapi xarray pandas eccodes cfgrib ecmwflibs netcdf4 ecmfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdsapi>=0.7.2 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: datapi in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cdsapi>=0.7.2) (0.1.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cdsapi>=0.7.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cdsapi>=0.7.2) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi>=0.7.2) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi>=0.7.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi>=0.7.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi>=0.7.2) (2023.7.22)\n",
      "Requirement already satisfied: attrs in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datapi->cdsapi>=0.7.2) (23.2.0)\n",
      "Requirement already satisfied: multiurl>=0.3.2 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datapi->cdsapi>=0.7.2) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datapi->cdsapi>=0.7.2) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sguti\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->cdsapi>=0.7.2) (0.4.6)\n",
      "Requirement already satisfied: pytz in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from multiurl>=0.3.2->datapi->cdsapi>=0.7.2) (2023.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from multiurl>=0.3.2->datapi->cdsapi>=0.7.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sguti\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil->multiurl>=0.3.2->datapi->cdsapi>=0.7.2) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install \"cdsapi>=0.7.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:04:45,295 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-04-16 15:04:45,298 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    }
   ],
   "source": [
    "client = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a single field with wget.\n",
    "\n",
    "Possible using the HTTP Byte-Range request feature.\n",
    "\n",
    "Example: Download temperature at 2m at step=24h from the 00 UTC HRES forecast on 25 January 2022.\n",
    "\n",
    "First download the associated index file by substituting the '.grib2' extension with '.index' in the URL.\n",
    "\n",
    "```\n",
    "wget [ROOT]/20240301/00z/ifs/0p25/oper/20240301000000-24h-oper-fc.index\n",
    "```\n",
    "\n",
    "Inspect the index file and look for the entry for 2m temperature ('param': '2t')\n",
    "\n",
    "```\n",
    "...\n",
    "{\"domain\": \"g\", \"date\": \"20240301\", \"time\": \"0000\", \"expver\": \"0001\", \"class\": \"od\", \"type\": \"fc\", \"stream\": \"oper\", \"step\": \"24\", \"levtype\": \"sfc\", \"param\": \"2t\", \"_offset\": 17459800, \"_length\": 609046}\n",
    "...\n",
    "```\n",
    "\n",
    "Use the values of \"_offset\" and \"_length\" keys for this record to construct the start bytes and end bytes:\n",
    "\n",
    "```\n",
    "start_bytes = __offset\n",
    "end_bytes == __offset + __length - 1\n",
    "```\n",
    "\n",
    "The \"_offset\" and \"_length\" values of a specific field will change from forecast run to forecast run.  It is necessary to redo this computation for each download.\n",
    "\n",
    "Use the start_bytes and end_bytes values calculated to pass the range of bytes to be downloaded to wget, this time for the \".grib2\" file:\n",
    "\n",
    "```\n",
    "wget [ROOT]/20240301/00z/ifs/0p25/oper/20240301000000-24h-oper-fc.grib2 --header=\"Range: bytes=17459800-18068845\"\n",
    "```\n",
    "\n",
    "Alternatively, curl can be used:\n",
    "\n",
    "```\n",
    "curl --range  17459800-18068845 [ROOT]/20240301/00z/ifs/0p25/oper/20240301000000-24h-oper-fc.grib2 --output 2t.grib2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  113M    0  254k    0     0   148k      0  0:12:59  0:00:01  0:12:58  149k\n",
      "  2  113M    2 2526k    0     0   965k      0  0:02:00  0:00:02  0:01:58  967k\n",
      "  4  113M    4 5278k    0     0  1454k      0  0:01:19  0:00:03  0:01:16 1456k\n",
      "  7  113M    7 8382k    0     0  1815k      0  0:01:03  0:00:04  0:00:59 1817k\n",
      "  9  113M    9 10.9M    0     0  1986k      0  0:00:58  0:00:05  0:00:53 2239k\n",
      " 11  113M   11 13.4M    0     0  2075k      0  0:00:55  0:00:06  0:00:49 2744k\n",
      " 14  113M   14 16.1M    0     0  2165k      0  0:00:53  0:00:07  0:00:46 2791k\n",
      " 16  113M   16 18.8M    0     0  2242k      0  0:00:51  0:00:08  0:00:43 2816k\n",
      " 19  113M   19 21.8M    0     0  2322k      0  0:00:50  0:00:09  0:00:41 2790k\n",
      " 21  113M   21 24.5M    0     0  2371k      0  0:00:48  0:00:10  0:00:38 2806k\n",
      " 24  113M   24 27.4M    0     0  2421k      0  0:00:47  0:00:11  0:00:36 2880k\n",
      " 26  113M   26 30.0M    0     0  2429k      0  0:00:47  0:00:12  0:00:35 2830k\n",
      " 28  113M   28 32.7M    0     0  2444k      0  0:00:47  0:00:13  0:00:34 2785k\n",
      " 31  113M   31 35.3M    0     0  2477k      0  0:00:46  0:00:14  0:00:32 2776k\n",
      " 33  113M   33 37.6M    0     0  2464k      0  0:00:47  0:00:15  0:00:32 2660k\n",
      " 35  113M   35 40.7M    0     0  2492k      0  0:00:46  0:00:16  0:00:30 2652k\n",
      " 38  113M   38 43.3M    0     0  2517k      0  0:00:46  0:00:17  0:00:29 2741k\n",
      " 40  113M   40 45.5M    0     0  2500k      0  0:00:46  0:00:18  0:00:28 2654k\n",
      " 42  113M   42 48.6M    0     0  2533k      0  0:00:45  0:00:19  0:00:26 2697k\n",
      " 45  113M   45 51.2M    0     0  2540k      0  0:00:45  0:00:20  0:00:25 2776k\n",
      " 47  113M   47 54.2M    0     0  2568k      0  0:00:45  0:00:21  0:00:24 2829k\n",
      " 50  113M   50 56.9M    0     0  2577k      0  0:00:45  0:00:22  0:00:23 2789k\n",
      " 52  113M   52 59.9M    0     0  2597k      0  0:00:44  0:00:23  0:00:21 2963k\n",
      " 55  113M   55 62.7M    0     0  2609k      0  0:00:44  0:00:24  0:00:20 2908k\n",
      " 57  113M   57 65.5M    0     0  2619k      0  0:00:44  0:00:25  0:00:19 2946k\n",
      " 60  113M   60 68.6M    0     0  2640k      0  0:00:43  0:00:26  0:00:17 2949k\n",
      " 62  113M   62 71.2M    0     0  2641k      0  0:00:43  0:00:27  0:00:16 2928k\n",
      " 65  113M   65 74.2M    0     0  2658k      0  0:00:43  0:00:28  0:00:15 2946k\n",
      " 67  113M   67 76.9M    0     0  2659k      0  0:00:43  0:00:29  0:00:14 2905k\n",
      " 70  113M   70 79.9M    0     0  2672k      0  0:00:43  0:00:30  0:00:13 2945k\n",
      " 73  113M   73 82.8M    0     0  2682k      0  0:00:43  0:00:31  0:00:12 2907k\n",
      " 75  113M   75 85.6M    0     0  2686k      0  0:00:43  0:00:32  0:00:11 2938k\n",
      " 77  113M   77 88.4M    0     0  2693k      0  0:00:43  0:00:33  0:00:10 2893k\n",
      " 80  113M   80 91.2M    0     0  2697k      0  0:00:43  0:00:34  0:00:09 2920k\n",
      " 83  113M   83 94.1M    0     0  2707k      0  0:00:42  0:00:35  0:00:07 2919k\n",
      " 85  113M   85 96.9M    0     0  2710k      0  0:00:42  0:00:36  0:00:06 2889k\n",
      " 87  113M   87 99.7M    0     0  2711k      0  0:00:42  0:00:37  0:00:05 2874k\n",
      " 90  113M   90  102M    0     0  2716k      0  0:00:42  0:00:38  0:00:04 2869k\n",
      " 92  113M   92  105M    0     0  2722k      0  0:00:42  0:00:39  0:00:03 2898k\n",
      " 95  113M   95  107M    0     0  2721k      0  0:00:42  0:00:40  0:00:02 2825k\n",
      " 97  113M   97  111M    0     0  2722k      0  0:00:42  0:00:41  0:00:01 2807k\n",
      "100  113M  100  113M    0     0  2729k      0  0:00:42  0:00:42 --:--:-- 2864k\n"
     ]
    }
   ],
   "source": [
    "! curl -o ./curl.grib2 \"https://data.ecmwf.int/forecasts/20250418/00z/ifs/0p25/oper/20250418000000-0h-oper-fc.grib2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 34686  100 34686    0     0  20002      0  0:00:01  0:00:01 --:--:-- 20049\n"
     ]
    }
   ],
   "source": [
    "! curl -o ./index.grib2 \"https://data.ecmwf.int/forecasts/20250418/00z/ifs/0p25/oper/20250418000000-0h-oper-fc.index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.ecmwf.int/forecasts/20250418/00z/ifs/0p25/oper/20250418000000-0h-oper-fc.grib2\n"
     ]
    }
   ],
   "source": [
    "root = 'https://data.ecmwf.int/forecasts/'\n",
    "yyyymmdd = '20250418'\n",
    "hh = '00' # values are 00, 06, 12, 18\n",
    "model = 'ifs'\n",
    "resol = '0p25' # 0.25 degree\n",
    "stream = 'oper' # high-resolution forecast\n",
    "step = '0' # 0h to 144h by 3h and 150h to 240h by 6h\n",
    "unit = 'h'\n",
    "type = 'fc' # forecast\n",
    "format = 'grib2' # or index\n",
    "\n",
    "path = root + yyyymmdd + \"/\" + hh + \"z/\" + model + \"/\" + resol + \"/\" + stream + \"/\" + yyyymmdd + hh + \"0000-\" + step + unit + \"-\" + stream + \"-\" + type + \".\" + format\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[ROOT]/[yyyymmdd]/[HH]z/[model]/[resol]/[stream]/[yyyymmdd][HH]0000-[step][U]-[stream]-[type].[format]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the GRIB file. Using 'with' will make sure to close the file before execution leaves the 'with' block.\n",
    "grib_filename = '20250412000000-0h-oper-fc.grib2'\n",
    "open = xr.open_dataset(grib_filename, engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': '2t'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.86593750000004\n"
     ]
    }
   ],
   "source": [
    "lat = 47.9 + 0.1\n",
    "long = 106.9 + 0.1\n",
    "kelvin = open['t2m'].sel(latitude=lat, longitude=long).data \n",
    "fahrenheit = (kelvin - 273.15) * 9/5 + 32\n",
    "print(fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of writing this, due to the recent CDS server migrations and slow queues for downloading the data, we can only download a dataset one month at a time. Thus, we will have to loop through every month and year we wish to download. **Important**: Make sure to change the range in 'years_list' to the decades you wish to download. For example, if you're downloading 1951 - 1960, then the range would be range(1951, 1961). In this notebook, I decided to download city data from Ulaanbaatar, Mongolia. The coordinates are 30.2672° N, 97.7431° W, respectively, or 30.3 and -97.7, respectively. **Important**: Since our dataset is specific to land data, it's likely that if the region includes any oceans or large bodies of water, the temperature recorded at those pixels will be inaccurate or missing.\n",
    "\n",
    "The link to the dataset: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview \n",
    "\n",
    "Dataset name: ERA5-Land hourly data from 1950 to present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.7 48.1 106.7 107.1\n",
      "['2024']\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30']\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28']\n"
     ]
    }
   ],
   "source": [
    "years_list = [str(year) for year in range(2024, 2025)]\n",
    "leap_years = [str(year) for year in range(2024, 2025, 4)]\n",
    "\n",
    "make_two_digits = ['01', '02', '03', '04', '05', '06', '07', '08', '09'] # API accepts single digit days in this format\n",
    "\n",
    "thirty_one = make_two_digits + [str(year) for year in range(10, 32)]\n",
    "thirty = make_two_digits + [str(year) for year in range(10, 31)]\n",
    "feb_days_leap = make_two_digits + [str(year) for year in range(10, 30)]\n",
    "feb_days_non_leap = make_two_digits + [str(year) for year in range(10, 29)]\n",
    "\n",
    "months_list = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11'] # API aceepts months in this format\n",
    "month_days = {\n",
    "\t'01':thirty_one,\n",
    "\t'02':feb_days_non_leap,\n",
    "\t'03':thirty_one,\n",
    "\t'04':thirty,\n",
    "\t'05':thirty_one,\n",
    "\t'06':thirty,\n",
    "\t'07':thirty_one,\n",
    "\t'08':thirty_one,\n",
    "\t'09':thirty,\n",
    "\t'10':thirty_one,\n",
    "\t'11':thirty,\n",
    "\t'12':thirty_one,\n",
    "}\n",
    "\n",
    "# Currently downloading data for a model for Mongolia. These are the coords for its capital Ulaanbaatar.\n",
    "city_long = 106.9\n",
    "city_lat = 47.9\n",
    "\n",
    "# Region boundaries for the capital. Will produce a 5x5 grid of values each time step. The spatial resolution of the dataset is 0.1° x 0.1°.\n",
    "bottom_lat = str(round(city_lat - 0.2, 1))\n",
    "top_lat = str(round(city_lat + 0.2, 1))\n",
    "left_long = str(round(city_long - 0.2, 1))\n",
    "right_long = str(round(city_long + 0.2, 1))\n",
    "\n",
    "print(bottom_lat, top_lat, left_long, right_long)\n",
    "print(leap_years)\n",
    "print(thirty_one)\n",
    "print(thirty)\n",
    "print(feb_days_leap)\n",
    "print(feb_days_non_leap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 16:12:04,636 INFO Request ID is e8d2a780-1d67-4f0e-90b2-fad1ec5459de\n",
      "2025-04-16 16:12:04,828 INFO status has been updated to accepted\n",
      "2025-04-16 16:12:13,799 INFO status has been updated to running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 49\u001b[0m\n\u001b[0;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreanalysis-era5-land\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2m_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     ],\n\u001b[0;32m     47\u001b[0m }\n\u001b[1;32m---> 49\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrib_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Open the GRIB file. Using 'with' will make sure to close the file before execution leaves the 'with' block.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mopen_dataset(grib_filename) \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# The GRIB file data contains mostly empty labels [number, surface, valid_time]. We'll drop them before we concatenate with the yearly dataset to save space.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sguti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datapi\\legacy_api_client.py:169\u001b[0m, in \u001b[0;36mLegacyApiClient.retrieve\u001b[1;34m(self, name, request, target)\u001b[0m\n\u001b[0;32m    167\u001b[0m submitted: Remote \u001b[38;5;241m|\u001b[39m Results\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_until_complete:\n\u001b[1;32m--> 169\u001b[0m     submitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_and_wait_on_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     submitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    175\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest,\n\u001b[0;32m    177\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sguti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datapi\\api_client.py:458\u001b[0m, in \u001b[0;36mApiClient.submit_and_wait_on_results\u001b[1;34m(self, collection_id, **request)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit_and_wait_on_results\u001b[39m(\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m, collection_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest: Any\n\u001b[0;32m    444\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m datapi\u001b[38;5;241m.\u001b[39mResults:\n\u001b[0;32m    445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Submit a request and wait for the results to be ready.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    datapi.Results\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sguti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datapi\\processing.py:489\u001b[0m, in \u001b[0;36mRemote.make_results\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, wait: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Results:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m--> 489\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_on_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_api_response(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sguti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datapi\\processing.py:469\u001b[0m, in \u001b[0;36mRemote._wait_on_results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_ready:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults not ready, waiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 469\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(sleep \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep_max)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yearly_dataset = []\n",
    "\n",
    "# IMPORTANT: Change 'ds_path' to wherever you originally store the data\n",
    "ds_path = 'C:/Users/sguti/preprocessing/Scorched Earth Data/'\n",
    "\n",
    "for year in years_list:\n",
    "    # Name of the NetCDF file to convert to after concatenating all monthly datasets\n",
    "    nc_filename = ds_path + 'Mongolia_2D_ERA5_' + year + '.nc'\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    for month in months_list:\n",
    "        # Take care of leap years\n",
    "        if month == '02' and year in leap_years:\n",
    "            month_days['02'] = feb_days_leap\n",
    "        else:\n",
    "            month_days['02'] = feb_days_non_leap\n",
    "        # Name of the raw monthly GRIB file\n",
    "        grib_filename = ds_path + 'Mongolia_2D_ERA5_' + year + '_' + month + '.grib'\n",
    "        dataset = 'reanalysis-era5-land'\n",
    "        request = {\n",
    "            'variable': [\n",
    "                '2m_temperature', \n",
    "                '2m_dewpoint_temperature', \n",
    "                'surface_pressure', \n",
    "                '10m_u_component_of_wind', \n",
    "                '10m_v_component_of_wind'\n",
    "            ],\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': month_days[month],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "            ],\n",
    "            'data_format': 'grib',\n",
    "            'download_format': 'unarchived',\n",
    "            'area': [\n",
    "                top_lat, left_long, bottom_lat, right_long,\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        client.retrieve(dataset, request, grib_filename)\n",
    "\n",
    "        # Open the GRIB file. Using 'with' will make sure to close the file before execution leaves the 'with' block.\n",
    "        with xr.open_dataset(grib_filename) as ds:\n",
    "            # The GRIB file data contains mostly empty labels [number, surface, valid_time]. We'll drop them before we concatenate with the yearly dataset to save space.\n",
    "            monthly_dataset = ds.drop(['number', 'surface', 'valid_time'], dim=None)\n",
    "            if len(yearly_dataset) == 0:\n",
    "                # If yearly_dataset is empty, there's nothing to concatenate with\n",
    "                yearly_dataset = monthly_dataset\n",
    "            else: \n",
    "                # We will concatenate the two datasets along the time dimension.\n",
    "                yearly_dataset = xr.concat([yearly_dataset, monthly_dataset], dim=\"time\")\n",
    "\n",
    "    end = timer()\n",
    "\n",
    "    print(\"Elapsed time in minutes for a year to download and process:\", ((end - start) / 60))\n",
    "\n",
    "    # Finally, save the yearly_dataset to a NetCDF file. While GRIB is the native format, GRIB data is generally “messier” than data in a self-describing format, such as NetCDF. \n",
    "    print(\"Storing to... \", nc_filename)\n",
    "    yearly_dataset.to_netcdf(nc_filename)\n",
    "    yearly_dataset = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
